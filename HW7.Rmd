---
title: "HW7"
author: "Xuan Wen Loo"
date: "2024-03-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

```{r}
library(ISLR2)
library(caret)
library(leaps)

k = 10
folds = sample(1:k,nrow(Boston),replace=TRUE)
flds = createFolds(Boston$medv, k = 10, list = TRUE, returnTrain = FALSE)

val.errors = matrix(NA, nrow = 12, ncol = k)

for (f in 1:k) {
  test_index = flds[[f]]
  
  test = Boston[test_index,]
  train = Boston[-test_index,]
  best.train = regsubsets(medv~.,data=train,nbest=1,nvmax=12)
  
  for(i in 1:12){
    test.mat = model.matrix(medv~.,data=test)
    
    coef.m = coef(best.train,id=i)
    
    pred = test.mat[,names(coef.m)]%*%coef.m
    val.errors[i, f] = mean((test$medv-pred)^2)
  }
}

mean_cv_errors = apply(val.errors, 1, mean)
optimal_model_size = which.min(mean_cv_errors)
cat("Optimal model size: ", optimal_model_size, "\n")
# obtain the predictors
full.reg = regsubsets(medv~., data = Boston, nvmax = 12, nbest = 1)
coef(full.reg, optimal_model_size) 
```

(b) To determine which predictors to be included in the model, I used 10-fold cross validation using subset selection to find the optimal model size. I first split the data into 10 folds, then each fold had a chance to be the test set and the test MSE (cross validation error) for each set is recorded. During each iteration of k-fold cross-validation, models with different subsets of predictors are evaluated, allowing for comparison of their MSE. Then, obtain the average cross validation error for each fold and choose the one with minimum cv error, which will give the optimal model size. After that, I performed the best subset selection on the full data set to obtain the predictors that should be included. I chose the final model with the smallest test MSE because it indicates that the model with that number of predictors is more likely to generalize well to unseen data.

(c) The assumptions needed to obtain the final model are the relationship between the predictors and the response variable should be linear; the residuals should be independent of each other; there should be a constant variance; the residuals should be normally distributed. Scatterplot of residuals against fitted values can help reveal patterns that violate the assumptions, while normal Q-Q plots can help to assess normality.

```{r}
optimal_predictors <- names(coef(full.reg, optimal_model_size))
final_model <- lm(medv ~ crim+zn+chas+nox+rm+dis+rad+tax+ptratio+lstat, data = Boston)
residuals <- residuals(final_model)

plot(final_model$fitted.values, residuals,
     xlab = "Fitted values",
     ylab = "Residuals",
     main = "Residuals vs Fitted Values Plot")

qqnorm(residuals)
qqline(residuals)
```

(d) Potential issues related to the final model may include multicollinearity among predictors, outliers influencing the model excessively, or influential data points skewing the results. To address the issue, vif() can be called on the model to detect multicollinearity.

(e) Bias and variance of the model can be estimated through techniques like cross-validation. In this case, cross-validation was used to estimate the average test MSE, which provides an indication of both bias and variance. However, obtaining separate measures of bias and variance directly from the model itself might not be straightforward, as these concepts are more commonly assessed in the context of model performance rather than directly from model coefficients.

## Problem 2

(a) The models in Part 2 and Part 3 are not equivalent because they approach the predictor, gender, differently. In Part 2, a single model is fitted with gender included as a dummy variable. This allows us to estimate the overall effect of gender on healthcare charges while controlling for other predictors like age and BMI. The model considers the possibility that gender influences charges differently across age and BMI groups, allowing for interaction effects between gender and other predictors.While in Part 3, separate models are fitted for males and females. This treats gender as a categorical predictor with no interaction effect. It assumes that the relationships between age, BMI, and charges are the same for both genders, but with potentially different intercepts and coefficients.

```{r}
# (b)
insurance <- read.csv("./insurance.csv")
data_male <- insurance[insurance$gender == "male", ]
data_female <- insurance[insurance$gender == "female", ]

fit_males <- lm(charges ~ age + bmi, data = data_male)
fit_females <- lm(charges ~ age + bmi, data = data_female)

summary(fit_males)$coefficients
summary(fit_females)$coefficients
```

```{r}
# (c)
model_without_interaction <- lm(charges ~ age + bmi + gender, data = insurance)
model_with_interaction <- lm(charges ~ age * bmi + gender, data = insurance)
anova(model_without_interaction, model_with_interaction)
```

## Problem 3

(a) Multicollinearity can be a problem for making accurate predictions. When the predictors in a model are highly correlated, it becomes difficult for the model to estimate the individual effects of each predictor on the response variable accurately. It will be challenging for the model to distinguish the unique contribution of each predictor as they are redundant in explaining the variation in the response variable. This can lead to unstable coefficient estimates and inflated standard errors, making it challenging to interpret the relationship between predictors and the response variable.

```{r}
# (b)
set.seed(42)
x1 = runif(100)
x2 = 0.8*x1 + rnorm(100,0,0.1)
Y = 3 + 2 * x1 + 4 * x2 + rnorm(100, 0, 2)
cor(x1, x2)
```

```{r}
# (c)
train_indices = sample(1:100, 70)  # 70% training data
train = data.frame(Y = Y[train_indices], x1 = x1[train_indices], x2 = x2[train_indices])
test = data.frame(Y = Y[-train_indices], x1 = x1[-train_indices], x2 = x2[-train_indices])

# Train the model
model = lm(Y ~ x1 + x2, data = train)

# Predict on test set
pred = predict(model, newdata = test)
test_mse = mean((test$Y - pred)^2)
test_mse
```

```{r}
# (d)
test_mses = numeric(2500)

for (i in 1:2500) {
  Y = 3 + 2 * x1 + 4 * x2 + rnorm(100, 0, 2)
  train_indices = sample(1:100, 70)
  train = data.frame(Y = Y[train_indices], x1 = x1[train_indices], x2 = x2[train_indices])
  test = data.frame(Y = Y[-train_indices], x1 = x1[-train_indices], x2 = x2[-train_indices])
  model = lm(Y ~ x1 + x2, data = train)
  pred = predict(model, newdata = test)
  test_mses[i] <- mean((test$Y - pred)^2)
}

mean_test_mse = mean(test_mses)
cat("The mean test MSE in this setting when the predictors are highly correlated is ", mean_test_mse, "\n")
hist(test_mses, breaks = 30, main = "Histogram of Test MSEs (Multicollinearity)", xlab = "Test MSE")
```

(d) The histogram appears to follow a roughly normal distribution, centered around a mean test MSE of approximately 4.166. This distribution suggests that most of the test MSE values fall within a relatively narrow range around the mean, indicating consistency in the model's predictive performance across different iterations of the simulation.

```{r}
# (e)
set.seed(24)
x1 <- runif(100)
x2 <- rnorm(100, 0, 1)
cor(x1, x2)
```

```{r}
# (f)
test_mses_uncorrelated <- numeric(2500)

for (i in 1:2500) {
  # Generate new Y's
  Y <- 3 + 2 * x1 + 4 * x2 + rnorm(100, 0, 2)
  train_indices <- sample(1:100, 70)
  train <- data.frame(Y = Y[train_indices], x1 = x1[train_indices], x2 = x2[train_indices])
  test <- data.frame(Y = Y[-train_indices], x1 = x1[-train_indices], x2 = x2[-train_indices])
  model <- lm(Y ~ x1 + x2, data = train)
  predictions <- predict(model, newdata = test)
  test_mses_uncorrelated[i] <- mean((test$Y - predictions)^2)
}

mean_test_mse_uncorrelated <- mean(test_mses_uncorrelated)
cat("The mean test MSE in this setting when the predictors are not correlated is ", mean_test_mse_uncorrelated, "\n")
hist(test_mses_uncorrelated, breaks = 30, main = "Histogram of Test MSEs (No Multicollinearity)", xlab = "Test MSE")

```

(g) Both histograms for highly correlated and not correlated predictors look similar. This suggests that the presence or absence of multicollinearity did not significantly impact the predictive performance of the model in this particular simulation study. Probably because the model used in the simulation study may be relatively simple, and thus not sensitive enough to detect differences between the highly correlated and not correlated predictor scenarios.